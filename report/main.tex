\documentclass{article}
\usepackage{listings}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{code}{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{codeblue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\definecolor{codegray}{gray}{0.9}
\definecolor{codeblue}{rgb}{0.0, 0.0, 0.5}

\lstset{style=code}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Operations Reserach 2 Final Thesis}
\author{Youssef Ben Khalifa}

\begin{document}
\maketitle

\section{Introduction}
The Traveling Salesman Problem (TSP) stands as one of the most iconic and extensively studied problems in combinatorial optimization and computer science. Despite its deceptively simple formulation—finding the shortest 
possible route that visits each city exactly once and returns to the origin city—the TSP has profound implications for both theoretical computer science and practical applications.

In this thesis we will go over the implementation of the main heuristics involving the famous TSP problem. The entire implementation is done using the C programming language and the 
CPLEX optimization LP optimization framework.
The goal is to research and implement the most common Metaheuristics and Mathheuristics for solving the TSP problem using MIP problems.

Throughout this thesis, we will explore the historical development of each optimization method discussed. We will trace the evolution of metaheuristics from early construction heuristics to 
sophisticated approaches like 
Tabu Search and Variable Neighborhood Search, examining how these methods emerged in response to computational challenges. 
Similarly, we will investigate the historical context behind mathheuristics, which represent a 
more recent fusion of mathematical programming with heuristic frameworks, developed to address the 
limitations of pure exact methods when tackling large-scale combinatorial problems. This historical perspective will 
provide valuable insights into why particular techniques were developed and how they have evolved to their current form.


\subsection{The TSP Problem and Its Significance}
The TSP belongs to the class of NP-hard problems, meaning that no known algorithm can solve it optimally in polynomial time as the problem size increases. 
This fundamental hardness has made the TSP a benchmark problem for evaluating algorithmic approaches and computational methods. The significance of the TSP extends far beyond its theoretical interest, with applications 
spanning numerous domains:
\begin{itemize}
	\item Logistics and transportation planning
	\item Circuit board drilling in manufacturing
	\item DNA sequencing in bioinformatics
	\item Vehicle routing and delivery optimization
	\item Telecommunications network design
	\item Warehouse order picking
\end{itemize}

The universality of the TSP's structure—finding an optimal arrangement or sequence—makes it relevant to any field where minimizing traversal costs is essential.

\subsection{Metaheuristics for TSP}
Given the computational intractability of finding exact solutions for large TSP instances, researchers have developed various metaheuristics—high-level problem-independent algorithmic frameworks 
that provide strategies for developing heuristic optimization algorithms. Unlike exact methods, metaheuristics sacrifice the guarantee of optimality for computational efficiency, providing near-optimal solutions in reasonable time frames.

Metaheuristics for the TSP include:
\begin{itemize}
	\item Construction heuristics like Nearest Neighbor and Greedy algorithms that build tours incrementally
	\item Local search methods such as 2-opt and 3-opt that iteratively improve solutions
	\item Population-based approaches like Genetic Algorithms that evolve multiple solutions simultaneously
	\item Memory-based techniques such as Tabu Search that use historical information to guide the search
	\item Adaptive methods like GRASP (Greedy Randomized Adaptive Search Procedure) that combine greedy elements with randomization
\end{itemize}

These approaches have proven remarkably effective in practice, often producing solutions within a few percentage points of optimality for instances with thousands of cities.

\subsection{Mathheuristics for TSP}
Mathheuristics represent a more recent development in the field, combining mathematical programming techniques with metaheuristic frameworks. 
These hybrid methods leverage the strengths of exact optimization methods (like linear and integer programming) while maintaining the scalability advantages of heuristics.

For the TSP, mathheuristics typically involve:
\begin{itemize}
	\item Decomposition techniques that break the problem into manageable subproblems
	\item Variable fixing approaches that reduce the problem size by fixing certain decision variables
	\item Local branching methods that explore neighborhoods defined by mathematical constraints
	\item Cut generation procedures that iteratively strengthen mathematical formulations
\end{itemize}

By integrating mathematical rigor with heuristic flexibility, mathheuristics often achieve superior performance compared to either approach used independently, 
especially for complex or large-scale instances.

\subsection{Scope of This Thesis}
This thesis investigates both traditional metaheuristics and modern mathheuristics for solving the TSP, implementing and analyzing several 
approaches using the C programming language and the CPLEX optimization framework. We focus on understanding the mathematical foundations, implementation details, 
and computational performance of these methods, with particular attention to their practical effectiveness across different problem instances.

Through systematic experimentation and analysis, we aim to provide insights into the strengths and limitations of different solution approaches, contributing to the 
ongoing effort to develop more efficient algorithms for this foundational optimization problem.

Throught the next chapters we will be going over following:
\begin{enumerate}
	\item \textbf{Introduction to the TSP Problem}: quick overview of the Mathematical formulation of the TSP problem.
	\item \textbf{Heuristics}: Implementation of common heuristic algorithms including:
		\begin{itemize}
			\item Greedy Randomized Adaptive Search (GRASP)
			\item Extra Mileage
			\item Refining Heuristics (2-opt move)
		\end{itemize}
	\item \textbf{Neighbourhood Search}: Implementation of neighbourhood search algorithms including:
		\begin{itemize}
			\item Tabu Search
			\item Variable Neighborhood Search (VNS)
		\end{itemize}
	\item \textbf{Mathheuristics}: Implementation of mathematical optimization techniques combined with heuristics:
		\begin{itemize}
			\item Hard Fixing
			\item Local Branching
		\end{itemize}
\end{enumerate}

For each algorithm, we will examine the implementation details, mathematical foundations, and effectiveness in solving the TSP problem. 
The implementations for the mathheuristic algorithms are done utilizing the CPLEX optimization framework. 

\newpage

\section{The Traveling Salesman Problem}
The Traveling Salesman Problem (TSP) is one of the most extensively studied problems in combinatorial optimization. The problem asks to find the shortest possible route that visits each city exactly once and returns to the origin city, 
given a set of cities and the distances between them.
Formally, given a set of $n$ cities and a cost matrix $[cij]$ that specifies the cost (or distance) of traveling from city $i$ to city $j$, the goal is to find a permutation of the cities that minimizes the total tour length. 
Despite its simple description, the TSP is NP-hard, meaning there is no known polynomial-time algorithm that can solve it optimally for large instances.

\subsection{Mathematical Formulation}
First let us define the components that characterize the TSP problem: we begin from a graph $G = (V, E)$ where $V$ is the set of vertices (cities) and $E$ is the set of edges connecting the vertices, and a cost matrix $C$ containing the weights/costs of each edge $(i, j) \in E$. 
Spoiler: in the implementation instead of adopting a big cost matrix, we will be simply using the euclidean distance between the nodes for computing the cost $c_{i, j}$ of a given edge $(i, j)$ , therefore the cost of a tour will simply be computed as the sum of the euclidean distances between the nodes. This will not affect the final formulation.   

We then can formalize Traveling Salesman Problem  as an optimization problem as follows:
\begin{equation}
	\min \sum_{i=1}^{n} \sum_{j=1}^{n} c_{ij} x_{ij}
\end{equation}
subject to:
\begin{equation}
	\sum_{i=1}^{n} x_{ij} = 1 \quad \forall j \in \{1, \ldots, n\}
\end{equation}
\begin{equation}
	u_i - u_j + nx_{ij} \leq n-1 \quad \forall i \in \{2, \ldots, n\}, j \in \{2, \ldots, n\}
\end{equation}
\begin{equation}
	x_{ij} \in \{0, 1\} \quad \forall i, j \in \{1, \ldots, n\}
\end{equation}
where:
\begin{itemize}
	\item $c_{ij}$ is the cost of traveling from node $i$ to node $j$.
	\item $x_{ij}$ is a binary variable that indicates whether the salesman travels from node $i$ to node $j$.
	\item $u_i$ is a continuous variable that represents the position of node $i$ in the tour.
\end{itemize}


\newpage

\section{Metaheuristics for TSP}
Metaheuristics represent a sophisticated class of computational methods that have evolved to address complex optimization problems where exact methods become impractical due to computational constraints. 
As optimization problems grow in scale and complexity, traditional exact methods often struggle with the combinatorial explosion of possible solutions. 
Metaheuristics emerged as an effective approach to navigate these vast solution spaces by intelligently balancing exploration of new regions with exploitation of promising areas.

\subsection{Historical Development}

In this section we will go over the implementation of the following metaheuristics:
\begin{itemize}
	\item Greedy Randomized Adaptive Search (GRASP)
	\item Extra Mileage
	\item Refining Heuristics (2-opt move)
	\item Tabu Search
	\item Variable Neighborhood Search (VNS)
\end{itemize}

\subsection{Greedy Randomized Adaptive Search (GRASP)}
The GRASP method is basically a greedy randomized adaptive search procedure.
It consists of two main phases: construction and local search.
In the construction phase, a feasible solution is built, one element at a time, in a greedy randomized fashion.
In the local search phase, the neighborhood of the constructed solution is explored until a local minimum is found.

\paragraph{Construction Phase}
In the construction phase, the solution is built iteratively. At each iteration, a candidate list is created,
containing the best elements to be added to the solution. One of these elements is chosen randomly, according
to a probability distribution, and added to the solution.

\paragraph{Local Search Phase}
In the local search phase, the algorithm explores the neighborhood of the constructed solution to find a local minimum.
This is done by iteratively replacing the current solution with a better solution from its neighborhood,
until no better solution can be found.

\paragraph{Algorithm}
The GRASP algorithm can be summarized as follows:
\begin{enumerate}
	\item \textbf{Initialization:} Set the best solution found to null.
	\item \textbf{Construction:} Build a feasible solution using the greedy randomized approach.
	\item \textbf{Local Search:} Improve the constructed solution using local search.
	\item \textbf{Update:} If the improved solution is better than the best solution found, update the best solution.
	\item \textbf{Termination:} Repeat steps 2-4 until a stopping criterion is met (e.g., a maximum number of iterations).
    \item 
\end{enumerate}


\paragraph{Implementation}
The implementation of the GRASP algorithm takes as input arguments:
\begin{itemize}
	\item \texttt{instance}: The TSP instance data structure.
	\item \texttt{start\_node}: The index of the starting node for the tour.
\end{itemize}
The function internally allocates and sets the solution as an array integers representing the tour....
The GRASP algorithm is implemented as follows:
\begin{enumerate} 
	\item \textbf{Setup}:
	      \begin{itemize}
		      \item The function initializes the current node index to the starting node (which is passed as an argument) 
			  and sets the remaining nodes count to the total number of nodes.
		      \item It allocates memory for the solution array and remaining nodes arrays. These two arrays will be used respectively to store the tour and the remaining nodes for the main loop to visit.
		      \item The solution array is initialized with \texttt{-1} to indicate that no nodes have been visited yet.
		      \item The remaining nodes array is initialized with all node indices.
	      \end{itemize}

	\item \textbf{Main Loop}:
	      \begin{itemize}
		      \item The function iterates over all nodes to construct the solution.
		      \item For each node, it finds the nearest unvisited node using the \texttt{euclidean\_nearest\_node} function.
		      \item If no nearest node is found (i.e., all nodes are visited), it connects the current node back to the starting node to complete the tour.
		      \item Otherwise, it updates the solution with the nearest node and logs the current node index and nearest node found.
		      \item The current node index is updated to the nearest node for the next iteration.
	      \end{itemize}
		

	\item \textbf{Finalization}:
	      \begin{itemize}
		      \item After constructing the solution, the function records the end time and calculates the elapsed time.
		      \item It logs the total time taken and the cost of the solution.
		      \item Finally, it frees the allocated memory for the remaining nodes array.
	      \end{itemize}
\end{enumerate}

Here's the pseudocode of the implementation:

\begin{algorithm}[!ht]
\caption{TSP GRASP}
\begin{algorithmic}[1]
\Procedure{TSP\_GRASP}{instance, solution, alpha}
\State solution $\gets \emptyset$
\State candidates $\gets$ \{1, ..., n\} \Comment{Set of unvisited nodes}
\State current $\gets$ RandomNode(candidates)
\State solution.Add(current)
\State Remove(candidates, current)

\While{candidates not empty}
    \State RCL $\gets \emptyset$ \Comment{Restricted Candidate List}
    \State $c_{min} \gets$ MinDistance(current, candidates)
    \State $c_{max} \gets$ MaxDistance(current, candidates)
    \State threshold $\gets c_{min} + \alpha(c_{max} - c_{min})$
    
    \ForAll{node $\in$ candidates}
        \If{Distance(current, node) $\leq$ threshold}
            \State RCL.Add(node)
        \EndIf
    \EndFor
    
    \State next $\gets$ RandomSelect(RCL)
    \State solution.Add(next)
    \State Remove(candidates, next)
    \State current $\gets$ next
\EndWhile

\State solution.Add(solution[0]) \Comment{Return to starting node}
\State \Return solution
\EndProcedure
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Extra Mileage}
In the extra mileage heuristic, we try to build a valid tsp solution starting an edge of the graph and we try and build a 
tour by selecting the adjacent edge with the minimum cost.

\paragraph{Algorithm}
The extra mileage algorithm can be summarized as follows:
\begin{enumerate}
	\item \textbf{Initialization:} Initialize the starting edge.
	\item \textbf{Construction:} Build a feasible solution by iteratively selecting the edge with the minimum cost.
	\item \textbf{Update:} If the improved solution is better than the best solution found, update the best solution.
	\item \textbf{Termination:} Repeat steps 2-3 until a stopping criterion is met (e.g., a maximum number of iterations).
\end{enumerate}

\paragraph{Implementation}
The extra mileage algoithm implementation takes as input arguments:
\begin{itemize}
	\item \texttt{instance}: The TSP instance data structure.
	\item \texttt{starting\_pair}: The pair of nodes that will be used to start the tour.
\end{itemize}
Ideally the starting pair should be the most distant pair of nodes in the graph in order to optimize the construction of the solution,
by design this is not a restriction that has been instrisically implemented into the function, so any pair of nodes can be used as a starting pair.
The extra mileage algorithm is implemented as follows:
\begin{enumerate} 
	\item \textbf{Setup}:
	\begin{itemize}
		\item The function initializes the current\_pair to the starting pair (which is passed as an argument).
		\item An heuristic state data stcuture is initialized to keep the track of the covered nodes and the final solution 
	\end{itemize}
	\item \textbf{Main Loop}:
	      \begin{itemize}
		      \item While there are uncovered nodes (condition based on the comparison of the covered nodes count and the total number of nodes), 
			  the function iterates over the uncovered nodes to construct the solution:
		            \begin{itemize}
			            \item For each covered node, a second loop is exectued to find the nearest uncovered node such that 
			            the the resulting edge between the two nodes is the argument that minimizes the total cost of the tour up until that point.
			            \item if such node is found, that node is selected and added to the tour, 
						the covered nodes count is incremented and the node is removed from the uncovered nodes list.
		            \end{itemize}
	      \end{itemize}
		
	\item \textbf{Finalization}:
	      \begin{itemize}
		      \item Once all nodes are covered, the function calculates the total cost of the solution.
		      \item It records the end time and calculates the elapsed time.
	      \end{itemize}
\end{enumerate}

\begin{algorithm}[!ht]
\caption{TSP Extra Mileage}
\begin{algorithmic}[1]
\Procedure{TSP\_ExtraMileage}{instance, solution, start\_pair}
\State solution $\gets \emptyset$
\State candidates $\gets$ \{1, ..., n\} \Comment{Set of unvisited nodes}
\State tour $\gets$ [start\_pair.first, start\_pair.second] \Comment{Initial tour with farthest pair}
\State Remove(candidates, start\_pair.first)
\State Remove(candidates, start\_pair.second)

\While{candidates not empty}
    \State best\_node $\gets$ null
    \State best\_pos $\gets$ null
    \State min\_cost $\gets \infty$

    \ForAll{node $\in$ candidates}
        \ForAll{pos $\in$ \{1, ..., |tour|\}}
            \State extra\_dist $\gets$ Distance(tour[pos-1], node)
            \State extra\_dist $\gets$ extra\_dist + Distance(node, tour[pos])
            \State extra\_dist $\gets$ extra\_dist - Distance(tour[pos-1], tour[pos])
            
            \If{extra\_dist < min\_cost}
                \State min\_cost $\gets$ extra\_dist
                \State best\_node $\gets$ node
                \State best\_pos $\gets$ pos
            \EndIf
        \EndFor
    \EndFor

    \State Insert(tour, best\_node, best\_pos)
    \State Remove(candidates, best\_node)
\EndWhile

\State solution $\gets$ tour
\State \Return solution
\EndProcedure
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Refining Heuristics}
Refining heuristics are a type of heuristic algorithm that iteratively improve an initial solution by applying a series of local search moves. These can be applied on top of the existing heuristics we have 
introduced so far in order to improve their performance. In the next section we will go over a one particular refining heuristic called the \textit{2-opt move}\cite{Heuristics_for_the_Traveling_Salesman_Problem}.

\subsubsection{2-opt Move}
Assume we have a graph $G = (V, E)$ and a directed tour $T \subseteq E$, valid as a tsp solution, 
the 2-opt move takes two edges in that tour and swaps the nodes 
between them.  In particular, let $(i, j), (h, k) \in T$, the 2-opt move replaces the edges $(i, j)$ and $(h, k)$ with the edges $(i, h)$ and $(j, k)$. \\
\\
\paragraph{Algorithm}
The goal of this type of operation is to swap two "longer" edges for two "shorter" ones (by "longer" and "shorter" we refer to the edge cost): 
therefore in our refining heuristic we aim at:
\begin{enumerate}
	\item Selecting two edges $(i, j), (h, k) \in T$: for selecting the two edges we can adopt two different strategies:
	\begin{itemize}
		\item Select the first pair of edges that satisfy the following:
		\begin{equation}
			c_{G}(i, j) + c_{G}(h, k) > c_{G}(i, h) + c_{G}(j, k)
			\label{eq:2-opt-move}
		\end{equation}
		\item Or select the pair of edges in $G$ which swap results in the maximum cost reduction:
		\begin{equation}
			\textit{argmax}_{((i, j), (h, k)) \in E \times E} (\Delta c_G = (c_G(i, h) + c_G(j, k)) - (c_G(i, j) + c_G(h, k)))
			\label{eq:2-opt-move-argmax}
		\end{equation}
	\end{itemize}
	
	where $c_G(i, j)$ is the cost associated with edge $(i, j)$.
	\item Replacing the two edges with the edges $(i, h)$ and $(j, k)$.
	\item If the graph is directed, invert the direction of the tour of the edges between $(j, k)$ and $(i, h)$.
\end{enumerate}
\paragraph{Implementation}
The 2-opt move implementation takes as input arguments:
\begin{itemize}
	\item \texttt{instance}: an instance of the TSP problem so solve.
	\item \texttt{solution}: an instance of a valid TSP solution.
\end{itemize}
The function is implemented as follows:
\begin{enumerate}
	\item \textbf{Main loop:} 
	\begin{itemize}
	\item The function iterates over all edges in the solution to find a valid 2-opt move. In this implementations, this is identified by the pair of edges that satisfy (\ref{eq:2-opt-move-argmax})
	\item If a valid 2-opt move is found, the pair is saved as the designated edges to be swapped.
	\end{itemize}
	\item \textbf{Swap:} We apply the 2-opt move on the solution by swapping the designated edges.
	\item \textbf{Finalization:} We invert the direction of the edges between $(j, k)$ and $(i, h)$ if the graph is directed.
\end{enumerate}
here is the pseudocode of the implementation:
\begin{algorithm}[!ht]
\caption{TSP 2-opt Local Search}
\begin{algorithmic}[1]
\Procedure{TSP\_TwoOpt}{instance, solution}
\State improved $\gets$ true
\While{improved}
    \State improved $\gets$ false
    \State best\_gain $\gets$ 0
    \State best\_i $\gets$ -1
    \State best\_j $\gets$ -1

    \For{i $\gets$ 0 \textbf{to} n-2}
        \For{j $\gets$ i+1 \textbf{to} n-1}
            \State gain $\gets$ CalculateTwoOptGain(i, j)
            \If{gain < best\_gain}
                \State best\_gain $\gets$ gain
                \State best\_i $\gets$ i
                \State best\_j $\gets$ j
                \State improved $\gets$ true
            \EndIf
        \EndFor
    \EndFor

    \If{improved}
        \State ReversePath(solution, best\_i+1, best\_j)
    \EndIf
\EndWhile

\State \Return solution
\EndProcedure

\Function{CalculateTwoOptGain}{i, j}
    \State old\_dist $\gets$ Distance(tour[i], tour[i+1])
    \State old\_dist $\gets$ old\_dist + Distance(tour[j], tour[j+1])
    \State new\_dist $\gets$ Distance(tour[i], tour[j])
    \State new\_dist $\gets$ new\_dist + Distance(tour[i+1], tour[j+1])
    \State \Return new\_dist - old\_dist
\EndFunction
\end{algorithmic}
\end{algorithm}

\newpage

\section{Neighbourhood Search Heuristics}
Neighbourhood serach algorithms are a family of algorithms whose search strategy is based moving from one neighbour solution to another. These neighbours are 

\subsection{Tabu Search}
The Tabu search is a type of neighborhood search algorithm. 
Differently with what happened with greedy algorithms like the GRASP, the Tabu search algorithm is able to escape local optima by allowing the search to move to worse solutions,
which can help the algorithm explore new regions of the search space and potentially find better solutions.
The main idea behind the Tabu search heuristic is to exclude the already explored solutions from the search space. 
This is done by maintaining a tabu list that stores the solutions that have been visited recently. 
The tabu list is used to prevent the search from revisiting the same solutions, 
which can help the algorithm escape local optima and explore new regions of the search space\cite{Heuristics_for_the_Traveling_Salesman_Problem}.

\paragraph{Algorithm}
The Tabu search algorithm can be summarized as follows:
\begin{enumerate}
    \item \textbf{Initialization:} Initialize the tabu list and the best solution found.
    \item \textbf{Setup:} Initialize the current solution and the current iteration count.
    \item \textbf{Main Loop:} Repeat the following steps until a stopping criterion is met:
          \begin{itemize}
              \item Generate a set of candidate solutions by applying a set of moves to the current solution.
              \item Select the best candidate solution that is not in the tabu list.
              \item Update the tabu list with the selected solution.
              \item Update the current solution with the selected solution.
              \item If the selected solution is better than the best solution found, update the best solution.
              \item Increment the iteration count.
          \end{itemize}
    \item \textbf{Finalization:} Return the best solution found.
\end{enumerate}


\begin{algorithm}[!ht]
\caption{TSP Tabu Search}
\begin{algorithmic}[1]
\Procedure{TSP\_TabuSearch}{instance, initial\_solution, tabu\_size}
\State best\_solution $\gets$ initial\_solution
\State best\_cost $\gets$ ComputeCost(best\_solution)
\State current\_solution $\gets$ initial\_solution
\State tabu\_list $\gets$ EmptyQueue(tabu\_size)
\State iterations $\gets$ 0

\While{iterations < MAX\_ITERATIONS}
    \State best\_move $\gets$ null
    \State best\_gain $\gets \infty$
    
    \ForAll{i, j $\in$ PossibleMoves(current\_solution)}
        \If{(i, j) $\notin$ tabu\_list \textbf{or} AspirationCriteria()}
            \State gain $\gets$ CalculateSwapGain(i, j)
            \If{gain < best\_gain}
                \State best\_gain $\gets$ gain
                \State best\_move $\gets$ (i, j)
            \EndIf
        \EndIf
    \EndFor
    
    \State ApplyMove(current\_solution, best\_move)
    \State current\_cost $\gets$ ComputeCost(current\_solution)
    \State AddToTabuList(tabu\_list, best\_move)
    
    \If{current\_cost < best\_cost}
        \State best\_solution $\gets$ current\_solution
        \State best\_cost $\gets$ current\_cost
    \EndIf
    
    \State iterations $\gets$ iterations + 1
\EndWhile

\State \Return best\_solution
\EndProcedure
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Variable Neighbourhood Search}
The Variable Neighbourhood Search (VNS) heuristic is a metaheuristic that combines local search with a systematic change of neighbourhood structures. 
The idea is to explore different neighbourhoods of the current solution to escape local optima and find better solutions.
Many formulations and version for the VNS algorithm have been proposed in the literature, such as the \textit{Basic VNS}, \textit{Reduced VNS} and \textit{General VNS}.\cite{VariableNeighborhood_Search}\\

All of these forumlations are based on a neighbourhood search, which is defined within a solution space that can be explored using the \textit{two-opt move}, 
\textit{three-opt move} or the \textit{k-opt move}, depending on the general scheme of the algorithm. These operations that can be made in the solution space for the TSP problem allow to generate a neighborhood
starting from a given solution, each of which neighbour differ from one single \textit{2-opt move}\cite{Heuristics_for_the_Traveling_Salesman_Problem}. 

\paragraph{Algorithm}
In our imeplementation we will go over the General VNS scheme, in which the neighbours are simply generated from a single \textit{two-opt move}, and the \textit{shaking function} 
The VNS algorithm can be summarized as follows:
\begin{enumerate}
	\item \textbf{Initialization:} Initialize the current solution and the best solution found.
	\item \textbf{Setup:} Initialize the neighbourhood structure and the current iteration count.
	\item \textbf{Main Loop:} Repeat the following steps until a stopping criterion is met:
		  \begin{itemize}
			  \item Apply a local search algorithm to the current solution.
			  \item Generate a new solution by applying a perturbation to the current solution.
			  \item If the new solution is better than the current solution, update the current solution.
			  \item If the current solution is better than the best solution found, update the best solution.
			  \item Change the neighbourhood structure.
			  \item Increment the iteration count.
		  \end{itemize}
	\item \textbf{Finalization:} Return the best solution found.
\end{enumerate}
\begin{algorithm}
\caption{TSP Variable Neighborhood Search}
\begin{algorithmic}[1]
\Procedure{TSP\_VNS}{instance, initial\_solution, k\_max}
\State best\_solution $\gets$ initial\_solution
\State best\_cost $\gets$ ComputeCost(best\_solution)
\State k $\gets$ 1

\While{k $\leq$ k\_max}
    \State current\_solution $\gets$ best\_solution
    \State // Shaking phase
    \State current\_solution $\gets$ Shake(current\_solution, k)
    
    \State // Local search phase
    \State improved $\gets$ true
    \While{improved}
        \State improved $\gets$ false
        \State neighbor $\gets$ LocalSearch(current\_solution, k)
        \State neighbor\_cost $\gets$ ComputeCost(neighbor)
        
        \If{neighbor\_cost < ComputeCost(current\_solution)}
            \State current\_solution $\gets$ neighbor
            \State improved $\gets$ true
        \EndIf
    \EndWhile

    \If{ComputeCost(current\_solution) < best\_cost}
        \State best\_solution $\gets$ current\_solution
        \State best\_cost $\gets$ ComputeCost(current\_solution)
        \State k $\gets$ 1
    \Else
        \State k $\gets$ k + 1
    \EndIf
\EndWhile

\State \Return best\_solution
\EndProcedure

\Function{Shake}{solution, k}
    \For{i $\gets$ 1 \textbf{to} k}
        \State pos1, pos2 $\gets$ RandomPositions(solution.length)
        \State SwapNodes(solution, pos1, pos2)
    \EndFor
    \State \Return solution
\EndFunction
\end{algorithmic}
\end{algorithm}
\newpage
\section{TSP with CPLEX}

\subsection{Brief introduction to CPLEX}
CPLEX is a high-performance optimization solver developed by IBM that can be used to solve linear programming (LP), mixed-integer programming (MIP), and quadratic programming (QP) problems.
It provides a powerful API that allows users to model and solve optimization problems in a variety of programming languages, including C, C++, Java, and Python.

In our case we will be using the C API to model and solve the TSP problem using CPLEX. The CPLEX C API provides a set of functions that allow users to create and manipulate optimization models, 
set parameters, and solve the models.

\subsection{Modeling the TSP with CPLEX}
To model the TSP with CPLEX, we need to define the decision variables, constraints, and objective function of the problem. 
The decision variables represent the edges of the graph, and the constraints ensure that each node is visited exactly once in the tour.
The objective function is to minimize the total cost of the tour.

\paragraph{CPLEX Environment}
The first step in using CPLEX is to create an environment object that will be used to manage the optimization process.
The environment object is created using the \texttt{CPXopenCPLEX} function, which returns a pointer to the CPLEX environment.

\begin{lstlisting}[language=C]
	CPXENVptr env = CPXopenCPLEX(&status);
\end{lstlisting}

\paragraph{Decision Variables}
To define decision variables in the CPLEX environment we use the \texttt{CPXnewcols} function, which creates a set of new columns (variables) in the model.
Each variable represents an edge in the graph and is binary (0 or 1) to indicate whether the edge is included in the tour.
\begin{lstlisting}[language=C]
	int num_edges = inst->nnodes * inst->nnodes;
	double* lb = (double*)malloc(num_edges * sizeof(double));
	double* ub = (double*)malloc(num_edges * sizeof(double));
	char* xctype = (char*)malloc(num_edges * sizeof(char));
	char** colnames = (char**)malloc(num_edges * sizeof(char*));
	for (int i = 0; i < num_edges; i++) {
		lb[i] = 0.0;
		ub[i] = 1.0;
		xctype[i] = 'B';
		colnames[i] = (char*)malloc(100 * sizeof(char));
		sprintf(colnames[i], "x_%d_%d", i / inst->nnodes, i % inst->nnodes);
	}
	status = CPXnewcols(env, lp, num_edges, NULL, lb, ub, xctype, colnames);
\end{lstlisting}

\subsection{Bender's subtour elimination method} 
Up until this point we have just modelled a generic integer linear problem into cplex, but this does not solve the TSP problem.
In order to have CPLEX return feasible solutions to the TSP problem, we need to add constraints that eliminate subtours in the solution.
This can be done using the Bender's subtour elimination method, which is a cutting-plane algorithm that adds constraints to the model to eliminate subtours.

\paragraph{Subtour Elimination Constraints}
The subtour elimination constraints are defined as follows: 
\begin{equation}
	\sum_{i \in S} \sum_{j \in S} x_{ij} \leq |S| - 1 \quad \forall S \subset V, 2 \leq |S| \leq |V| - 1
\end{equation}
where $V$ is the set of nodes in the graph and $S$ is a subset of nodes that forms a subtour.

\paragraph{Implementation}
The implementation of the Bender's subtour elimination method involves adding the subtour elimination constraints to the model and solving the model iteratively until no subtours are found.
This is can be done in two ways:
\begin{enumerate}
    \item \textbf{Iterative Approach:} Solve the model, check for subtours in the solution, and add constraints for each subtour found. This process continues until a solution with no subtours is obtained. The steps are:
    \begin{itemize}
        \item Solve the initial TSP model
        \item Find connected components in the solution graph
        \item If multiple components exist, add subtour elimination constraints
        \item Repeat until only one component remains
    \end{itemize}
    
    \item \textbf{CPLEX Callback Approach:} Utilize CPLEX's lazy constraint callback mechanism to add subtour elimination constraints during the optimization process. This approach:
    \begin{itemize}
        \item Registers a callback function with CPLEX
        \item CPLEX calls this function whenever it finds a new integer feasible solution
        \item The callback checks for subtours and adds necessary constraints immediately
        \item More efficient as it integrates with CPLEX's branch-and-cut framework
    \end{itemize}
\end{enumerate}

\paragraph{Iterative Approach}
Here is the implementation of the iterative approach to the Bender's subtour elimination method:
\begin{enumerate} 
	\item \textbf{Setup}:
	      \begin{itemize}
		      \item The function initializes the current node index to the starting node (which is passed as an argument) 
			  and sets the remaining nodes count to the total number of nodes.
		      \item It allocates memory for the solution array and remaining nodes arrays. These two arrays will be used respectively to store the tour and the remaining nodes for the main loop to visit.
		      \item The solution array is initialized with \texttt{-1} to indicate that no nodes have been visited yet.
		      \item The remaining nodes array is initialized with all node indices.
	      \end{itemize}

		\begin{lstlisting}[language=C]
			clock_t start_time = clock();
			int current_node_index = starting_node;
			int remaining_nodes_count = inst->nnodes;
			solution nearest_node;
			int* remaining_nodes = (int*)malloc(inst->nnodes * sizeof(int));
			inst->solution = (int*)malloc(inst->nnodes * sizeof(int));
			inst->best_cost_value = 0;
			for (int i = 0; i < inst->nnodes; i++) {
				inst->solution[i] = -1;
				remaining_nodes[i] = i;
			}
			remaining_nodes[current_node_index] = remaining_nodes[--remaining_nodes_count];
		\end{lstlisting}

	\item \textbf{Main Loop}:
	      \begin{itemize}
		      \item The function iterates over all nodes to construct the solution.
		      \item For each node, it finds the nearest unvisited node using the \texttt{euclidean\_nearest\_node} function.
		      \item If no nearest node is found (i.e., all nodes are visited), it connects the current node back to the starting node to complete the tour.
		      \item Otherwise, it updates the solution with the nearest node and logs the current node index and nearest node found.
		      \item The current node index is updated to the nearest node for the next iteration.
	      \end{itemize}
		  \begin{lstlisting}[language=C]
			do
			{
				xstar = TSPopt(instance, env, lp);
				init_data_struct(instance, &component_map, &succ, &ncomp);
				build_solution(xstar, instance, solution, component_map, ncomp);
				error = add_bender_constraint(env, lp, NULL, component_map, instance, *ncomp);
			}while (*ncomp > 1);
		  \end{lstlisting}

	\item \textbf{Finalization}:
	      \begin{itemize}
		      \item After constructing the solution, the function records the end time and calculates the elapsed time.
		      \item It logs the total time taken and the cost of the solution.
		      \item Finally, it frees the allocated memory for the remaining nodes array.
	      \end{itemize}
		\begin{lstlisting}[language=C]
			clock_t end_time = clock();
			double elapsed_time = ((double) (end_time - start_time)) / CLOCKS_PER_SEC;
			inst->elapsed_time = elapsed_time;
			inst->best_cost_value = compute_solution_cost(inst, inst->solution);
		\end{lstlisting}
\end{enumerate}

\paragraph{CPLEX Callback Approach}
Here is the implementation of the CPLEX callback approach to the Bender's subtour elimination method:
\begin{enumerate}
	\item \textbf{Setup}:
	\begin{itemize}
		\item Initialize the CPLEX environment and the TSP instance data structure.
		\item Create and build the TSP CPLEX model.
		\item Register the lazy constraint callback function with CPLEX using the \texttt{CPXcallbacksetfunc} function provided by the CPLEX API.
	\end{itemize}
	\begin{lstlisting}[language=C]
		int error = 0;
		CPXENVptr env = CPXopenCPLEX(&error);
		if (error) print_error("CPXopenCPLEX() error");
		CPXLPptr lp = CPXcreateprob(env, &error, "TSP model version 1");
		if (error) print_error("CPXcreateprob() error");
		double lower_bound = -CPX_INFBOUND;
		double upper_bound = CPX_INFBOUND;

		CPXsetintparam(env, CPX_PARAM_SCRIND, CPX_OFF);
		if (_verbose >= 60) CPXsetintparam(env, CPX_PARAM_SCRIND, CPX_ON);
		CPXsetintparam(env, CPX_PARAM_RANDOMSEED, 1);
		CPXsetdblparam(env, CPX_PARAM_TILIM, 36000);
		CPXsetintparam(env, CPX_PARAM_CUTUP, upper_bound);

		build_model(instance, env, lp);
		if (contextid == NULL) contextid = CPX_CALLBACKCONTEXT_CANDIDATE;
		if (CPXcallbacksetfunc(env, lp, contextid, callback_driver, instance)) print_error("CPXcallbacksetfunc() error");
	\end{lstlisting}
	\item \textbf{Lazy Constraint Callback Function}:
	\begin{itemize}
		\item The lazy constraint callback function is called by CPLEX whenever a new integer feasible solution is found. (Note: this solution may not be a valid TSP tour);
		\item The solution is built and checked for subtours using the \texttt{build\_solution} function.
		\item The function checks the number of connected components in the solution and adds necessary constraints to eliminate them.
		\item It uses the \texttt{CPXcutcallbackadd} function to add the subtour elimination constraints to the model.
		\item The callback function returns control to CPLEX after adding the constraints.
	\end{itemize}
	\begin{lstlisting}[language=C]
		
	\end{lstlisting}
	\item \textbf{Main Loop}:
	\begin{itemize}
		\item Solve the model using CPLEX and the lazy constraint callback function.
		\item CPLEX will call the callback function whenever a new integer feasible solution is found.
		\item The callback function will add subtour elimination constraints to the model.
		\item Repeat until CPLEX returns a valid TSP solution, that is a solution such that the number of subtours is equal to 1.
	\end{itemize}
	\item \textbf{Finalization}:
	\begin{itemize}
		\item After solving the model, the function records the end time and calculates the elapsed time.
		\item It logs the total time taken and the cost of the solution.
		\item Finally, it frees the allocated memory for the solution and the TSP instance data structure.
	\end{itemize}
\end{enumerate}

\subsection{Patching Heuristic}

\newpage

\section{Mathheuristics}
In the field of Mathematical optimization, one of the most tackled issues is the efficiency and the performance with which
we are able to solve MIP problems. With time, both solver and the hardware have evolved to the point where we are able to solve even some large scale problems 
in a reasonable amount of time. However, there are still some problems that are too large to be solved in a reasonable amount of time, and this is where the Mathheuristics 
come into play. Mathheuristics are a combination of mathematical optimization techniques and heuristics that are used to improve the efficiency and performance of 
already existing mathematical optimization algorithms.\cite{Fischetti2003LocalBranching}\cite{Fischetti2016Matheuristics} \\
In this section we will go over the implementation of the following Mathheuristics:
\begin{itemize}
	\item \textbf{Hard Fixing}
	\item \textbf{Local Branching}
\end{itemize}

Mathheuristics algorithms are meant ot be used in concatenation with a "black box" solver, meaning that these algorithms are not directly implemented directly into the solver, but thery are 
applied onto the input instance that is about to be fed into the solver. The idea is to improve the overall performance of the solver by optimizing the search space for solver to explore.\cite{Fischetti2016Matheuristics}

\subsection{Hard Fixing Mathheuristic}
The idea behind Fixed Branching is to optimize the solution space search by adding a set of constraints to the model that restrict the 
search space to a local region around the current solution. These constraints are designed to select a random subset of edges in the current solution, 
so as to create a restricted neighborhood of feasible solutions. 
This allows the solver to explore a smaller search space and potentially find better solutions faster. 

\subparagraph{Algorithm}
The Hard Fixing algorithm can be summarized as follows: 
\begin{enumerate}
	\item fix a small random probability $\rho \in (0, 1)$ (i.e. for the implementation we went for $\rho = 0.3$);
	\item extract the subset of edges to be fixed based on the probability $\rho$: these edges are again selected at random from the subset of edges that are not selected in the current solution;
	\begin{equation}
		\bar{\mathcal{S}} = \{ (i, j) \in \mathcal{S} : (i, j) = 0, |\bar{\mathcal{S}}| = \rho |\mathcal{B}|\}
	\end{equation}
	where $\mathcal{S} \subseteq \mathcal{B}$ and $\mathcal{B}$ is the set of binary variables. 
	To put it in words, we want to select a subset of edges that are not selected in the current solution, and we want to fix them to 1 with a probability $\rho$.
	\item add constraints to the model to fix the selected subset of edges to 1.
\end{enumerate}
This process is repeated every time a new integer feasible solution is found, and the constraints are added to the model to restrict the search space. It is implicitly 
understood that the local branching constraints are reset at every iteration of the algorithm. 

\subparagraph{Implementation}
The algorithm is implemented using the CPLEX Callback functionality, which allows us to add constraints to the model during the optimization process. Every time a new integer feasible solution is found, the callback function is called, 
and we can add constraints to the model to restrict the search space. 
In the callback function, the cplex\_hard\_fixing function is called, which fixes a subset of variables in the model based on a given probability p\_fix.

The function does the following:
\begin{enumerate}
	\item \textbf{Update incumbent}: Get the current solution from the callback context
	\begin{lstlisting}
		int error = CPXcallbackgetcandidatepoint(context, xstar, 0, ncols - 1, NULL);
	\end{lstlisting}
	\item \textbf{Restore to original instance}: Reset the bounds of all variables to their original values (0.0 to 1.0).
	\begin{lstlisting}
		
	\end{lstlisting}
	\item Fix a subset of variables based on the given probability p\_fix.
	\item Apply the fixing by adding constraints to the model.
\end{enumerate}

\begin{lstlisting}
int cplex_hard_fixing(instance* instance, CPXCALLBACKCONTEXTptr context, double p_fix)
{
    int ncols = instance->ncols;
    double* xstar = (double*)calloc(ncols, sizeof(double));
    int* indices = (int*)calloc(ncols, sizeof(int));
    double* bd = (double*)calloc(ncols, sizeof(double));
    char* lu = (char*)calloc(ncols, sizeof(char));

    // Initialize random seed
    srand(time(NULL));

    // Get the current solution
    int error = CPXcallbackgetcandidatepoint(context, xstar, 0, ncols - 1, NULL);
    if (error)
    {
        free(xstar);
        free(indices);
        free(bd);
        free(lu);
        return error;
    }

    // Reset the bounds of all variables to their original values (0.0 to 1.0)
    for (int i = 0; i < ncols; i++)
    {
        indices[i] = i;
        lu[i] = 'B'; // Both lower and upper bounds
        bd[i] = 0.0; // Lower bound
    }
    error = CPXcallbackpostheursoln(context, ncols, indices, bd, 0.0, CPXCALLBACKSOLUTION_PROPAGATE);
    if (error)
    {
        free(xstar);
        free(indices);
        free(bd);
        free(lu);
        return error;
    }

    for (int i = 0; i < ncols; i++)
    {
        bd[i] = 1.0; // Upper bound
    }

    error = CPXcallbackpostheursoln(context, ncols, indices, bd, 0.0, CPXCALLBACKSOLUTION_PROPAGATE);
    if (error)
    {
        free(xstar);
        free(indices);
        free(bd);
        free(lu);
        return error;
    }

    // Fix a subset of variables based on the given probability
    int fix_count = 0;
    for (int i = 0; i < ncols; i++)
    {
        if (((double)rand() / RAND_MAX) < p_fix)
        {
            indices[fix_count] = i;
            lu[fix_count] = 'L'; // Only lower bound
            bd[fix_count] = 1.0; // Fix to 1
            fix_count++;
        }
    }

    // Apply the fixing
    if (fix_count > 0)
    {
        error = CPXcallbackpostheursoln(context, fix_count, indices, bd, 0.0, CPXCALLBACKSOLUTION_PROPAGATE);
        if (error)
        {
            free(xstar);
            free(indices);
            free(bd);
            free(lu);
            return error;
        }
    }

    free(xstar);
    free(indices);
    free(bd);
    free(lu);
    return error;
}
\end{lstlisting}



\subsection{Local Branching}
As for the Hard fixing heuristic, the Local branching algorihtm is meant to be used with a heuristic black TSP solver to optimize its performance.
The goal of the Local Branching algorithm is to reduce the solution search space the black solver has to explore by "branching" from a given TSP solution $\bar{x}$ to a restricted neighborhood of $\bar{x}$.
The neighborhood is defined analougously to what we did with the VNS or the Tabu Search algorithm, where we adopted the idea of the \textit{k-opt Neighbourhood}: we therefore need to set a hyperparameter $k$
that determines the size of our neighborhood\cite{Fischetti2003LocalBranching}.

\paragraph{Algorithm}
Starting from a given solution $\hat{x}$, we add the following constraints to the model:
\begin{equation}
	\Delta(x , \hat{x}) := \sum_{j \in \mathcal{S}} (1 - x_j) + \sum_{j \in \mathcal{B} \setminus \mathcal{S}} x_j \leq k
	\label{eq:local_branching_constraint}
\end{equation} 
where $\mathcal{B}$ is the index set of the binary variables, and $\mathcal{S} := \{ j \in \mathcal{B} : \hat{x}_j = 1\}$\cite{Fischetti2003LocalBranching}.  
At each iteration, we then compute the set $\mathcal{S}$ we then compute \ref{eq:local_branching_constraint}, add it to the model and solve the model.
If the solution is feasible, we update the best solution found and repeat the process until a stopping criterion is met.

\paragraph{Implementation}
Starting from a given solution $\bar{x}$, and given the starting tsp instance, the Cplex programming environment and pointer, as shown in the function declaration below:
\begin{lstlisting}
	int add_local_branching_constraint(instance* inst, CPXENVptr env, CPXLPptr lp,CPXCALLBACKCONTEXTptr cpxcallbackcontex_tptr, const double* xstar, double k)
\end{lstlisting} 
we proceed with the floowing steps:
\begin{enumerate}
	\item \textbf{Constraint Modeling}: Starting from the given solution vector $xstar$, we build the constraint the will be later added to the CPLEX model.
	\begin{lstlisting}
		int ncols = CPXgetnumcols(env, lp);
		int* index = (int*)calloc(ncols, sizeof(int));
		double* coefficients = (double*)calloc(ncols, sizeof(double));
		double right_hand_side_value = k;
		char constraint_sense = 'G';
		char* rname = (char*)calloc(100, sizeof(char));
		sprintf(rname, "local_branching_constraint");

		int non_zero_variables_count = 0;
		for (int i = 0; i < ncols; i++)
		{
			if (xstar[i] > 0.8)
			{
				index[non_zero_variables_count] = i;
				coefficients[non_zero_variables_count] = 1.0;
				non_zero_variables_count++;
			}
		}

		right_hand_side_value = inst->nnodes - k;
	\end{lstlisting}
	\item \textbf{Constraint Addition}: We then add the constraint to the model: this can be done directly by adding a row directing into the model:
	\begin{lstlisting}
		CPXaddrows(env, lp, 0, 1, non_zero_variables_count, &right_hand_side_value, &constraint_sense,&izero,index,coefficients, NULL, &rname);
	\end{lstlisting}
	or in the case we are using the CPLEX callback function, we can use the \texttt{CPXcallbackaddusercuts} function to add the constraint to the model
	\begin{lstlisting}
		CPXcallbackrejectcandidate(cpxcallbackcontex_tptr, 1, non_zero_variables_count,&right_hand_side_value,&constraint_sense, &izero, index, coefficients);
	\end{lstlisting}
	\item \textbf{Model Resolution}: We then solve the model and check if the solution is feasible. If the solution is feasible, we update the best solution found, 
	remove the latest local Branching constrint (if necessary), and repeat the process until a stopping criterion is met.
\end{enumerate}

\subparagraph{Note}
The \ref{eq:local_branching_constraint} in CPLEX needs to be expressed using a particular form: we need to express the constraint
to be expressed as an inequality of the form $Ax \leq b$, where $A$ is a matrix of coefficients, $x$ is the vector of variables and $b$ is the right-hand side of the inequality.
In our case, we can express the constraint as follows:
\begin{align}
	& \sum_{j \in \mathcal{S}} (1 - x_j) \leq k \\
	\Rightarrow & \sum_{j \in \mathcal{S}}1 -\sum_{j \in \mathcal{S}} x_j \leq k' \\
	\Rightarrow & -\sum_{j \in \hat{\mathcal{S}}} x_j \leq k' - \sum_{j \in \mathcal{S}}1 \\
	\Rightarrow & \sum_{j \in \mathcal{S}} x_j \geq |\mathcal{S}| - k'
\end{align}
\newpage

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}